{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Support vector machine in Python\n",
    "\n",
    "There are many different packages in Python that can let you use different machine learning algorithms really easy. The most popular package for general machine learning is [scikit-learn](https://scikit-learn.org/stable/), which contains many different algorithms. There are also packages more towards deep learning, such as tensorflow, pytorch and so on, but we will not cover them here. In this chapter, we will only use scikit-learn to learn these basics. You can easily install scikit-learn use a package manager. \n",
    "\n",
    "Let's see an example how to use it. We start by loading some [pre-existing datasets](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.datasets) in the scikit-learn, which comes with a few standard datasets. For example, the [iris](https://en.wikipedia.org/wiki/Iris_flower_data_set) and [digits](http://archive.ics.uci.edu/ml/datasets/Pen-Based+Recognition+of+Handwritten+Digits) datasets for classification and the [boston house prices](http://archive.ics.uci.edu/ml/datasets/Housing) dataset for regression. Using these existing datasets, we can easily test the algorithms that we are interested in. We will use the iris dataset for this section.\n",
    "\n",
    "A dataset is a dictionary-like object that holds all the data and some metadata about the data. This data is stored in the .data member, which is a n_samples, n_features array. In the case of supervised problem, one or more target variables are stored in the .target member.\n",
    "\n",
    "**Load iris data**\n",
    "\n",
    "The iris dataset consists of 50 samples from each of three species of Iris (Iris setosa, Iris virginica and Iris versicolor). Four features were measured from each sample: the length and the width of the sepals and petals, in centimetres.\n",
    "\n",
    "\n",
    "| [![Iris Setosa](https://upload.wikimedia.org/wikipedia/commons/5/56/Kosaciec_szczecinkowaty_Iris_setosa.jpg)](https://en.wikipedia.org/wiki/Iris_setosa)  | [![Iris Virginica](https://upload.wikimedia.org/wikipedia/commons/thumb/9/9f/Iris_virginica.jpg/1920px-Iris_virginica.jpg)](https://en.wikipedia.org/wiki/Iris_virginica) | [![Iris Versicolor](https://upload.wikimedia.org/wikipedia/commons/2/27/Blue_Flag%2C_Ottawa.jpg)](https://en.wikipedia.org/wiki/Iris_versicolor) |\n",
    "|:---:|:---:|:---:|\n",
    "| Iris Setosa| Iris Virginica| Iris Versicolor|\n",
    "\n",
    "Now let's use scikit-learn to train a SVM model to classify the different species of Iris. In order to have a better visualization, we will only use two features that can characterize the differences between the classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "plt.style.use('seaborn-poster')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# import the iris data\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "print(iris.feature_names)\n",
    "# only print the first 10 samples\n",
    "print(iris.data[:10])\n",
    "print('We have %d data samples with %d \\\n",
    "    features'%(iris.data.shape[0], iris.data.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "The data is always a 2D array, shape (n_samples, n_features), although the original data may have had a different shape. The following prints out the target names and the representatoin of the target using 0, 1, 2. Each of them represent a class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "print(iris.target_names)\n",
    "print(set(iris.target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Let's prepare the feature matrix $X$ and also the target $y$ for our problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# let's just use two features, so that we can \n",
    "# easily visualize them\n",
    "X = iris.data[:, [0, 2]]\n",
    "y = iris.target\n",
    "target_names = iris.target_names\n",
    "feature_names = iris.feature_names\n",
    "# get the classes\n",
    "n_class = len(set(y))\n",
    "print('We have %d classes in the data'%(n_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "If we can, we always want to plot the data out first to explore it. We can plot it as a scatter plot with different symbols for different classes. We can see with this two features, we can actually see they separate out from each other. Also, the boundary between these classes are fairly linear, thus all we need to do is to find a linear boundary between them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# let's have a look of the data first\n",
    "colors = ['b', 'g', 'r']\n",
    "symbols = ['o', '^', '*']\n",
    "plt.figure(figsize = (10,8))\n",
    "for i, c, s in (zip(range(n_class), colors, symbols)):\n",
    "    ix = y == i\n",
    "    plt.scatter(X[:, 0][ix], X[:, 1][ix], \\\n",
    "                color = c, marker = s, s = 60, \\\n",
    "                label = target_names[i])\n",
    "\n",
    "plt.legend(loc = 2, scatterpoints = 1)\n",
    "plt.xlabel('Feature 1 - ' + feature_names[0])\n",
    "plt.ylabel('Feature 2 - ' + feature_names[2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "We now use the SVM in scikit-learn. The API is quite simple, for most of the algorithms they are similar. The use of the different algorithms are usually the following steps:\n",
    "\n",
    "**Step 1:** initialize the model\n",
    "**Step 2:** train the model using the *fit* function\n",
    "**Step 3:** predict on the new data using the *predict* function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Initialize SVM classifier\n",
    "clf = svm.SVC(kernel='linear')\n",
    "\n",
    "######################################\n",
    "## WRITE YOUR CODE HERE (10 Points) ## \n",
    "######################################\n",
    "\n",
    "# Hint: Use the `fit` function of the classifier to train the model with the feature matrix `X` and the target vector `y`.\n",
    "# Train the classifier with data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "The above print out from the *fit* function is the parameters used in the model, we can see that usually for a model there are many different parameters that you may need to tune. For SVM, two most important parameters are _C_ and *gamma*. We won't go into the details here, but a good advice is that before you use the model, always try to understand what these parameters are to get a good model. Now let's use the predict function on the training data, usually we don't do this, we need to split the data into training and testing dataset. For the testing dataset, which is not used in training at all, it is only saved for evaluation purposes. Here for simplicity, we just have a look of the results on the training data we used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# predict on the data\n",
    "clf.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "We can plot the decision boundary for the model. The following function plot the decision boundary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Plotting decision regions\n",
    "def plot_desicion_boundary(X, y, clf, title = None):\n",
    "    '''\n",
    "    Helper function to plot the decision boundary for the SVM\n",
    "    '''\n",
    "    \n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01),\n",
    "                         np.arange(y_min, y_max, 0.01))\n",
    "\n",
    "    ######################################\n",
    "    ## WRITE YOUR CODE HERE (15 Points) ## \n",
    "    ######################################\n",
    "\n",
    "    # Hint: Use the `predict` method of the trained classifier `clf` to predict the class for each point in the mesh grid. \n",
    "    # The mesh grid points have been flattened and combined into a single array (np.c_[xx.ravel(), yy.ravel()]).\n",
    "    \n",
    "    Z = #\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    plt.figure(figsize = (10, 8))\n",
    "    plt.contourf(xx, yy, Z, alpha=0.4)\n",
    "    \n",
    "    for i, c, s in (zip(range(n_class), colors, symbols)):\n",
    "        ix = y == i\n",
    "        plt.scatter(X[:, 0][ix], X[:, 1][ix], \\\n",
    "                    color = c, marker = s, s = 60, \\\n",
    "                    label = target_names[i])\n",
    "    \n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    \n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.show()\n",
    "    \n",
    "plot_desicion_boundary(X, y, clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "We can see the linear boundaries found by the SVM for the 3 classes are generally good, and can separate most of the samples. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39w",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
