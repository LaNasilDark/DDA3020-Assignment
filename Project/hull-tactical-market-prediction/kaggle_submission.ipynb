{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ec6217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded!\n",
      "LightGBM available: True\n",
      "Data directory: .\n",
      "Train shape: (9021, 98)\n",
      "Number of base features: 94\n",
      "Training samples: 3481\n",
      "\n",
      "==================================================\n",
      "CROSS-VALIDATION\n",
      "==================================================\n",
      "Fold 1: Val size=580, Baseline Sharpe=0.6155\n",
      "Fold 2: Val size=580, Baseline Sharpe=0.8586\n",
      "Fold 3: Val size=580, Baseline Sharpe=1.0483\n",
      "Fold 2: Val size=580, Baseline Sharpe=0.8586\n",
      "Fold 3: Val size=580, Baseline Sharpe=1.0483\n",
      "Fold 4: Val size=580, Baseline Sharpe=0.2079\n",
      "Fold 4: Val size=580, Baseline Sharpe=0.2079\n",
      "Fold 5: Val size=580, Baseline Sharpe=1.0223\n",
      "\n",
      "Baseline Sharpe: 0.7505 (+/- 0.3120)\n",
      "\n",
      "Optimizing Strategy...\n",
      "\n",
      "Optimized Parameters:\n",
      "  - Sensitivity: 1000\n",
      "  - Min Weight: 0.6\n",
      "  - Max Weight: 1.1\n",
      "\n",
      "Sharpe per fold: ['0.6228', '0.8691', '1.0941', '0.1451', '1.0540']\n",
      "Mean Sharpe: 0.7570\n",
      "Improvement: 0.9%\n",
      "\n",
      "==================================================\n",
      "TRAINING FINAL MODEL\n",
      "==================================================\n",
      "Total features: 102\n",
      "Fold 5: Val size=580, Baseline Sharpe=1.0223\n",
      "\n",
      "Baseline Sharpe: 0.7505 (+/- 0.3120)\n",
      "\n",
      "Optimizing Strategy...\n",
      "\n",
      "Optimized Parameters:\n",
      "  - Sensitivity: 1000\n",
      "  - Min Weight: 0.6\n",
      "  - Max Weight: 1.1\n",
      "\n",
      "Sharpe per fold: ['0.6228', '0.8691', '1.0941', '0.1451', '1.0540']\n",
      "Mean Sharpe: 0.7570\n",
      "Improvement: 0.9%\n",
      "\n",
      "==================================================\n",
      "TRAINING FINAL MODEL\n",
      "==================================================\n",
      "Total features: 102\n",
      "Model trained!\n",
      "\n",
      "Predict function defined!\n",
      "\n",
      "==================================================\n",
      "LOCAL TESTING\n",
      "==================================================\n",
      "Test data shape: (10, 99)\n",
      "\n",
      "✓ Submission saved!\n",
      "  Predictions: 10\n",
      "  Weight range: [0.8755, 1.0106]\n",
      "  Mean weight: 0.9271\n",
      "\n",
      "Predictions:\n",
      "   date_id  prediction\n",
      "0     8980    0.875494\n",
      "1     8981    0.894251\n",
      "2     8982    1.010592\n",
      "3     8983    0.915298\n",
      "4     8984    0.904581\n",
      "5     8985    0.976290\n",
      "6     8986    0.938991\n",
      "7     8987    0.906612\n",
      "8     8988    0.956115\n",
      "9     8989    0.892914\n",
      "\n",
      "==================================================\n",
      "SUMMARY\n",
      "==================================================\n",
      "Version: V15 - Simple High-Sensitivity Strategy\n",
      "Similar to version that achieved 3.751 score\n",
      "CV Mean Sharpe: 0.7570\n",
      "Model trained!\n",
      "\n",
      "Predict function defined!\n",
      "\n",
      "==================================================\n",
      "LOCAL TESTING\n",
      "==================================================\n",
      "Test data shape: (10, 99)\n",
      "\n",
      "✓ Submission saved!\n",
      "  Predictions: 10\n",
      "  Weight range: [0.8755, 1.0106]\n",
      "  Mean weight: 0.9271\n",
      "\n",
      "Predictions:\n",
      "   date_id  prediction\n",
      "0     8980    0.875494\n",
      "1     8981    0.894251\n",
      "2     8982    1.010592\n",
      "3     8983    0.915298\n",
      "4     8984    0.904581\n",
      "5     8985    0.976290\n",
      "6     8986    0.938991\n",
      "7     8987    0.906612\n",
      "8     8988    0.956115\n",
      "9     8989    0.892914\n",
      "\n",
      "==================================================\n",
      "SUMMARY\n",
      "==================================================\n",
      "Version: V15 - Simple High-Sensitivity Strategy\n",
      "Similar to version that achieved 3.751 score\n",
      "CV Mean Sharpe: 0.7570\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import os\n",
    "import warnings\n",
    "from scipy.optimize import minimize\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    HAS_LGB = True\n",
    "except ImportError:\n",
    "    HAS_LGB = False\n",
    "\n",
    "print('Libraries loaded!')\n",
    "print(f'LightGBM available: {HAS_LGB}')\n",
    "\n",
    "# ============================================\n",
    "# CONFIGURATION\n",
    "# ============================================\n",
    "MIN_INVESTMENT = 0\n",
    "MAX_INVESTMENT = 2\n",
    "TRADING_DAYS_PER_YEAR = 252\n",
    "\n",
    "if os.path.exists('/kaggle/input'):\n",
    "    DATA_DIR = '/kaggle/input/hull-tactical-market-prediction'\n",
    "else:\n",
    "    DATA_DIR = '.'\n",
    "\n",
    "print(f'Data directory: {DATA_DIR}')\n",
    "\n",
    "# ============================================\n",
    "# EVALUATION METRIC\n",
    "# ============================================\n",
    "def calculate_sharpe_ratio(positions, forward_returns, risk_free_rate):\n",
    "    positions = np.array(positions)\n",
    "    forward_returns = np.array(forward_returns)\n",
    "    risk_free_rate = np.array(risk_free_rate)\n",
    "    \n",
    "    strategy_returns = risk_free_rate * (1 - positions) + positions * forward_returns\n",
    "    strategy_excess_returns = strategy_returns - risk_free_rate\n",
    "    strategy_excess_cumulative = (1 + strategy_excess_returns).prod()\n",
    "    n = len(positions)\n",
    "    strategy_mean_excess_return = strategy_excess_cumulative ** (1 / n) - 1\n",
    "    strategy_std = strategy_returns.std()\n",
    "    \n",
    "    if strategy_std == 0:\n",
    "        return -999\n",
    "    \n",
    "    sharpe = strategy_mean_excess_return / strategy_std * np.sqrt(TRADING_DAYS_PER_YEAR)\n",
    "    strategy_volatility = strategy_std * np.sqrt(TRADING_DAYS_PER_YEAR) * 100\n",
    "    \n",
    "    market_excess_returns = forward_returns - risk_free_rate\n",
    "    market_excess_cumulative = (1 + market_excess_returns).prod()\n",
    "    market_mean_excess_return = market_excess_cumulative ** (1 / n) - 1\n",
    "    market_std = forward_returns.std()\n",
    "    market_volatility = market_std * np.sqrt(TRADING_DAYS_PER_YEAR) * 100\n",
    "    \n",
    "    if market_volatility == 0:\n",
    "        return -999\n",
    "    \n",
    "    excess_vol = max(0, strategy_volatility / market_volatility - 1.2)\n",
    "    vol_penalty = 1 + excess_vol\n",
    "    \n",
    "    return_gap = max(0, (market_mean_excess_return - strategy_mean_excess_return) * 100 * TRADING_DAYS_PER_YEAR)\n",
    "    return_penalty = 1 + (return_gap ** 2) / 100\n",
    "    \n",
    "    adjusted_sharpe = sharpe / (vol_penalty * return_penalty)\n",
    "    return adjusted_sharpe\n",
    "\n",
    "# ============================================\n",
    "# FEATURE ENGINEERING\n",
    "# ============================================\n",
    "class FeatureEngineer:\n",
    "    def __init__(self):\n",
    "        self.imputer = SimpleImputer(strategy='median')\n",
    "        self.scaler = RobustScaler()\n",
    "        self.feature_cols = None\n",
    "        self.all_feature_cols = None\n",
    "        \n",
    "    def _add_engineered_features(self, df):\n",
    "        df = df.copy()\n",
    "        m_cols = [c for c in df.columns if c.startswith('M')]\n",
    "        v_cols = [c for c in df.columns if c.startswith('V')]\n",
    "        s_cols = [c for c in df.columns if c.startswith('S')]\n",
    "        e_cols = [c for c in df.columns if c.startswith('E')]\n",
    "        \n",
    "        for prefix, cols in [('M', m_cols), ('V', v_cols), ('S', s_cols), ('E', e_cols)]:\n",
    "            if cols:\n",
    "                df[f'{prefix}_mean'] = df[cols].mean(axis=1)\n",
    "                df[f'{prefix}_std'] = df[cols].std(axis=1)\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    def fit(self, df, feature_cols):\n",
    "        self.feature_cols = feature_cols\n",
    "        df_eng = self._add_engineered_features(df)\n",
    "        eng_cols = [c for c in df_eng.columns if c.endswith(('_mean', '_std'))]\n",
    "        self.all_feature_cols = feature_cols + [c for c in eng_cols if c not in feature_cols]\n",
    "        \n",
    "        X = df_eng[self.all_feature_cols].values\n",
    "        X_imputed = self.imputer.fit_transform(X)\n",
    "        self.scaler.fit(X_imputed)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        df_eng = self._add_engineered_features(df)\n",
    "        X = df_eng[self.all_feature_cols].values\n",
    "        X_imputed = self.imputer.transform(X)\n",
    "        X_scaled = self.scaler.transform(X_imputed)\n",
    "        return X_scaled\n",
    "    \n",
    "    def fit_transform(self, df, feature_cols):\n",
    "        self.fit(df, feature_cols)\n",
    "        return self.transform(df)\n",
    "\n",
    "# ============================================\n",
    "# ROBUST STRATEGY - Optimized across multiple time windows\n",
    "# ============================================\n",
    "class RobustStrategy:\n",
    "    \"\"\"Strategy that uses averaged parameters across multiple validation windows.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.sensitivity = 1500\n",
    "        self.max_weight = 1.3\n",
    "        self.min_weight = 0.5\n",
    "        \n",
    "    def predict_to_weight(self, predictions):\n",
    "        predictions = np.array(predictions)\n",
    "        scaled = predictions * self.sensitivity\n",
    "        sigmoid_output = 1 / (1 + np.exp(-scaled))\n",
    "        weights = self.min_weight + (self.max_weight - self.min_weight) * sigmoid_output\n",
    "        weights = np.clip(weights, MIN_INVESTMENT, MAX_INVESTMENT)\n",
    "        return weights\n",
    "    \n",
    "    def optimize_across_windows(self, all_predictions, all_returns, all_rf_rates):\n",
    "        \"\"\"Optimize using averaged performance across multiple windows.\"\"\"\n",
    "        \n",
    "        def objective(params):\n",
    "            sensitivity, max_w, min_w = params\n",
    "            \n",
    "            # Constraints check\n",
    "            if min_w >= max_w: return 1e9\n",
    "            \n",
    "            sharpes = []\n",
    "            for preds, rets, rfs in zip(all_predictions, all_returns, all_rf_rates):\n",
    "                # Temporary predict_to_weight logic\n",
    "                scaled = preds * sensitivity\n",
    "                sigmoid_output = 1 / (1 + np.exp(-scaled))\n",
    "                weights = min_w + (max_w - min_w) * sigmoid_output\n",
    "                weights = np.clip(weights, MIN_INVESTMENT, MAX_INVESTMENT)\n",
    "                \n",
    "                sharpe = calculate_sharpe_ratio(weights, rets, rfs)\n",
    "                sharpes.append(sharpe)\n",
    "            \n",
    "            # Objective: Maximize (Mean - 0.5 * Std) -> Minimize negative\n",
    "            score = np.mean(sharpes) - 0.5 * np.std(sharpes)\n",
    "            return -score\n",
    "\n",
    "        # Initial guess\n",
    "        x0 = [1500, 1.3, 0.5] \n",
    "        \n",
    "        # Bounds\n",
    "        bounds = [\n",
    "            (500, 5000),  # sensitivity\n",
    "            (1.0, 2.0),   # max_weight\n",
    "            (0.0, 1.0)    # min_weight\n",
    "        ]\n",
    "        \n",
    "        print(\"Optimizing strategy parameters...\")\n",
    "        result = minimize(objective, x0, method='Nelder-Mead', bounds=bounds, tol=1e-4)\n",
    "        \n",
    "        best_params = result.x\n",
    "        self.sensitivity = best_params[0]\n",
    "        self.max_weight = best_params[1]\n",
    "        self.min_weight = best_params[2]\n",
    "        \n",
    "        # Calculate final stats\n",
    "        final_sharpes = []\n",
    "        for preds, rets, rfs in zip(all_predictions, all_returns, all_rf_rates):\n",
    "            weights = self.predict_to_weight(preds)\n",
    "            sharpe = calculate_sharpe_ratio(weights, rets, rfs)\n",
    "            final_sharpes.append(sharpe)\n",
    "            \n",
    "        return {\n",
    "            'sensitivity': self.sensitivity,\n",
    "            'max_weight': self.max_weight,\n",
    "            'min_weight': self.min_weight,\n",
    "            'sharpes': final_sharpes,\n",
    "            'mean_sharpe': np.mean(final_sharpes),\n",
    "            'std_sharpe': np.std(final_sharpes)\n",
    "        }\n",
    "\n",
    "# ============================================\n",
    "# ENSEMBLE MODEL\n",
    "# ============================================\n",
    "class EnsembleModel:\n",
    "    def __init__(self):\n",
    "        self.models = []\n",
    "        self.weights = []\n",
    "        \n",
    "    def _optimize_weights(self, predictions_list, y_true):\n",
    "        def objective(weights):\n",
    "            weights = np.array(weights)\n",
    "            weights = weights / np.sum(weights)\n",
    "            final_pred = np.zeros_like(predictions_list[0])\n",
    "            for i, w in enumerate(weights):\n",
    "                final_pred += w * predictions_list[i]\n",
    "            # Maximize correlation with target\n",
    "            return -np.corrcoef(final_pred, y_true)[0, 1]\n",
    "\n",
    "        n_models = len(predictions_list)\n",
    "        init_weights = [1.0 / n_models] * n_models\n",
    "        constraints = ({'type': 'eq', 'fun': lambda w: 1 - np.sum(w)})\n",
    "        bounds = [(0, 1) for _ in range(n_models)]\n",
    "        \n",
    "        result = minimize(objective, init_weights, method='SLSQP', bounds=bounds, constraints=constraints)\n",
    "        return list(result.x / np.sum(result.x))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Split for weight optimization\n",
    "        split_idx = int(len(X) * 0.8)\n",
    "        X_train, X_val = X[:split_idx], X[split_idx:]\n",
    "        y_train, y_val = y[:split_idx], y[split_idx:]\n",
    "        \n",
    "        if HAS_LGB:\n",
    "            models_configs = [\n",
    "                {'n_estimators': 300, 'max_depth': 4, 'learning_rate': 0.03, 'num_leaves': 15,\n",
    "                 'min_child_samples': 60, 'reg_alpha': 0.2, 'reg_lambda': 0.2,\n",
    "                 'subsample': 0.8, 'colsample_bytree': 0.8, 'random_state': 42},\n",
    "                {'n_estimators': 250, 'max_depth': 5, 'learning_rate': 0.04, 'num_leaves': 20,\n",
    "                 'min_child_samples': 50, 'reg_alpha': 0.15, 'reg_lambda': 0.15,\n",
    "                 'subsample': 0.75, 'colsample_bytree': 0.75, 'random_state': 123},\n",
    "                {'n_estimators': 200, 'max_depth': 6, 'learning_rate': 0.05, 'num_leaves': 25,\n",
    "                 'min_child_samples': 40, 'reg_alpha': 0.1, 'reg_lambda': 0.1,\n",
    "                 'subsample': 0.8, 'colsample_bytree': 0.8, 'random_state': 456},\n",
    "            ]\n",
    "            \n",
    "            self.models = [lgb.LGBMRegressor(**config, verbose=-1) for config in models_configs]\n",
    "        else:\n",
    "            self.models = [\n",
    "                GradientBoostingRegressor(n_estimators=200, max_depth=4, learning_rate=0.05,\n",
    "                                         min_samples_leaf=40, random_state=42),\n",
    "                RandomForestRegressor(n_estimators=200, max_depth=6, min_samples_leaf=30,\n",
    "                                     random_state=42, n_jobs=-1),\n",
    "                Ridge(alpha=10.0)\n",
    "            ]\n",
    "        \n",
    "        # Train on split\n",
    "        for model in self.models:\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "        # Optimize weights\n",
    "        val_preds = [model.predict(X_val) for model in self.models]\n",
    "        self.weights = self._optimize_weights(val_preds, y_val)\n",
    "        print(f\"Optimized weights: {[f'{w:.4f}' for w in self.weights]}\")\n",
    "        \n",
    "        # Refit on full data\n",
    "        for model in self.models:\n",
    "            model.fit(X, y)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions = np.zeros(X.shape[0])\n",
    "        for model, weight in zip(self.models, self.weights):\n",
    "            predictions += weight * model.predict(X)\n",
    "        return predictions\n",
    "\n",
    "# ============================================\n",
    "# TRAINING WITH MULTI-WINDOW VALIDATION\n",
    "# ============================================\n",
    "\n",
    "train_df = pd.read_csv(f'{DATA_DIR}/train.csv')\n",
    "print(f'Train shape: {train_df.shape}')\n",
    "\n",
    "target_cols = ['forward_returns', 'risk_free_rate', 'market_forward_excess_returns']\n",
    "id_col = 'date_id'\n",
    "exclude_cols = [id_col] + target_cols\n",
    "\n",
    "test_df_sample = pd.read_csv(f'{DATA_DIR}/test.csv', nrows=5)\n",
    "test_cols = set(test_df_sample.columns)\n",
    "\n",
    "feature_cols = [col for col in train_df.columns if col not in exclude_cols and col in test_cols]\n",
    "print(f'Number of base features: {len(feature_cols)}')\n",
    "\n",
    "# Filter training data\n",
    "missing_by_date = train_df[feature_cols].isnull().sum(axis=1)\n",
    "threshold = len(feature_cols) * 0.05\n",
    "valid_mask = missing_by_date <= threshold\n",
    "valid_start_idx = valid_mask.idxmax()\n",
    "valid_start_date = train_df.loc[valid_start_idx, 'date_id']\n",
    "train_clean = train_df[train_df['date_id'] >= valid_start_date].copy().reset_index(drop=True)\n",
    "print(f'Training samples after filtering: {len(train_clean)}')\n",
    "\n",
    "# Multi-window cross-validation\n",
    "print('\\n' + '='*50)\n",
    "print('MULTI-WINDOW CROSS-VALIDATION')\n",
    "print('='*50)\n",
    "\n",
    "n_splits = 5\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "all_predictions = []\n",
    "all_returns = []\n",
    "all_rf_rates = []\n",
    "window_sharpes = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(tscv.split(train_clean)):\n",
    "    train_data = train_clean.iloc[train_idx].copy()\n",
    "    val_data = train_clean.iloc[val_idx].copy()\n",
    "    \n",
    "    # Feature engineering\n",
    "    fe_fold = FeatureEngineer()\n",
    "    X_train = fe_fold.fit_transform(train_data, feature_cols)\n",
    "    y_train = train_data['forward_returns'].values\n",
    "    \n",
    "    X_val = fe_fold.transform(val_data)\n",
    "    \n",
    "    # Train model\n",
    "    model_fold = EnsembleModel()\n",
    "    model_fold.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    val_preds = model_fold.predict(X_val)\n",
    "    \n",
    "    all_predictions.append(val_preds)\n",
    "    all_returns.append(val_data['forward_returns'].values)\n",
    "    all_rf_rates.append(val_data['risk_free_rate'].values)\n",
    "    \n",
    "    # Calculate baseline sharpe for this window\n",
    "    baseline_sharpe = calculate_sharpe_ratio(\n",
    "        np.ones(len(val_data)),\n",
    "        val_data['forward_returns'].values,\n",
    "        val_data['risk_free_rate'].values\n",
    "    )\n",
    "    window_sharpes.append(baseline_sharpe)\n",
    "    \n",
    "    print(f'Fold {fold+1}: Val size={len(val_data)}, Baseline Sharpe={baseline_sharpe:.4f}, Pred range=[{val_preds.min():.6f}, {val_preds.max():.6f}]')\n",
    "\n",
    "print(f'\\nBaseline Sharpe across windows: {np.mean(window_sharpes):.4f} (+/- {np.std(window_sharpes):.4f})')\n",
    "\n",
    "# Optimize strategy across all windows\n",
    "print('\\nOptimizing strategy across all windows...')\n",
    "strategy = RobustStrategy()\n",
    "best_params = strategy.optimize_across_windows(all_predictions, all_returns, all_rf_rates)\n",
    "\n",
    "print(f'\\nOptimized Parameters:')\n",
    "print(f'  - Sensitivity: {strategy.sensitivity:.2f}')\n",
    "print(f'  - Max Weight: {strategy.max_weight:.2f}')\n",
    "print(f'  - Min Weight: {strategy.min_weight:.2f}')\n",
    "print(f'\\nSharpe per window: {[f\"{s:.4f}\" for s in best_params[\"sharpes\"]]}')\n",
    "print(f'Mean Sharpe: {best_params[\"mean_sharpe\"]:.4f}')\n",
    "print(f'Std Sharpe: {best_params[\"std_sharpe\"]:.4f}')\n",
    "\n",
    "# Final training on all data\n",
    "print('\\n' + '='*50)\n",
    "print('FINAL TRAINING')\n",
    "print('='*50)\n",
    "\n",
    "fe = FeatureEngineer()\n",
    "X_full = fe.fit_transform(train_clean, feature_cols)\n",
    "y_full = train_clean['forward_returns'].values\n",
    "print(f'Total features: {X_full.shape[1]}')\n",
    "\n",
    "model = EnsembleModel()\n",
    "model.fit(X_full, y_full)\n",
    "print('Model trained on full dataset!')\n",
    "\n",
    "# ============================================\n",
    "# PREDICTION FUNCTION\n",
    "# ============================================\n",
    "\n",
    "def predict(test_batch: pl.DataFrame) -> pl.DataFrame:\n",
    "    test_pd = test_batch.to_pandas()\n",
    "    \n",
    "    if 'date_id' in test_pd.columns:\n",
    "        row_ids = test_pd['date_id'].values\n",
    "    else:\n",
    "        row_ids = test_pd.iloc[:, 0].values\n",
    "    \n",
    "    X_test = fe.transform(test_pd)\n",
    "    predictions = model.predict(X_test)\n",
    "    weights = strategy.predict_to_weight(predictions)\n",
    "    weights = np.clip(weights, MIN_INVESTMENT, MAX_INVESTMENT)\n",
    "    \n",
    "    result = pl.DataFrame({\n",
    "        'date_id': row_ids,\n",
    "        'prediction': weights\n",
    "    })\n",
    "    \n",
    "    return result\n",
    "\n",
    "print('\\nPredict function defined!')\n",
    "\n",
    "# ============================================\n",
    "# KAGGLE INFERENCE SERVER\n",
    "# ============================================\n",
    "\n",
    "import kaggle_evaluation.core.templates\n",
    "from kaggle_evaluation.default_gateway import DefaultGateway\n",
    "\n",
    "class HullTacticalInferenceServer(kaggle_evaluation.core.templates.InferenceServer):\n",
    "    def __init__(self):\n",
    "        super().__init__(predict)\n",
    "    \n",
    "    def _get_gateway_for_test(self, data_paths=None, file_share_dir=None):\n",
    "        return DefaultGateway(data_paths)\n",
    "\n",
    "# ============================================\n",
    "# RUN\n",
    "# ============================================\n",
    "\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    print('Running in Kaggle competition mode...')\n",
    "    inference_server = HullTacticalInferenceServer()\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    print('\\n' + '='*50)\n",
    "    print('LOCAL TESTING')\n",
    "    print('='*50)\n",
    "    \n",
    "    test_df = pd.read_csv(f'{DATA_DIR}/test.csv')\n",
    "    test_pl = pl.from_pandas(test_df)\n",
    "    \n",
    "    submission = predict(test_pl)\n",
    "    submission_pd = submission.to_pandas()\n",
    "    \n",
    "    import pyarrow as pa\n",
    "    import pyarrow.parquet as pq\n",
    "    \n",
    "    table = pa.Table.from_pandas(submission_pd, preserve_index=False)\n",
    "    pq.write_table(table, 'submission.parquet')\n",
    "    submission_pd.to_csv('submission.csv', index=False)\n",
    "    \n",
    "    print(f'\\n✓ Submission saved!')\n",
    "    print(f'  Total predictions: {len(submission_pd)}')\n",
    "    print(f'  Weight range: [{submission_pd[\"prediction\"].min():.4f}, {submission_pd[\"prediction\"].max():.4f}]')\n",
    "    print(f'  Mean weight: {submission_pd[\"prediction\"].mean():.4f}')\n",
    "    print(f'\\nAll predictions:')\n",
    "    print(submission_pd)\n",
    "    \n",
    "    print(f'\\n' + '='*50)\n",
    "    print('FINAL SUMMARY')\n",
    "    print('='*50)\n",
    "    print(f'Cross-Validation Performance:')\n",
    "    print(f'  - Mean Sharpe: {best_params[\"mean_sharpe\"]:.4f}')\n",
    "    print(f'  - Std Sharpe: {best_params[\"std_sharpe\"]:.4f}')\n",
    "    print(f'  - Baseline Mean: {np.mean(window_sharpes):.4f}')\n",
    "    avg_improvement = (best_params[\"mean_sharpe\"] / np.mean(window_sharpes) - 1) * 100 if np.mean(window_sharpes) > 0 else 0\n",
    "    print(f'  - Average Improvement: {avg_improvement:.1f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
