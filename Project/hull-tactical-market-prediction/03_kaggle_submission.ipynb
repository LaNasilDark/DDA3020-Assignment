{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c18c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Try to import lightgbm (available on Kaggle)\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    HAS_LGB = True\n",
    "except ImportError:\n",
    "    HAS_LGB = False\n",
    "\n",
    "print('Libraries loaded!')\n",
    "print(f'LightGBM available: {HAS_LGB}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1acc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MIN_INVESTMENT = 0\n",
    "MAX_INVESTMENT = 2\n",
    "\n",
    "# Data paths - adjust for Kaggle environment\n",
    "import os\n",
    "if os.path.exists('/kaggle/input'):\n",
    "    # Kaggle environment\n",
    "    DATA_DIR = '/kaggle/input/hull-tactical-market-prediction'\n",
    "else:\n",
    "    # Local environment\n",
    "    DATA_DIR = '.'\n",
    "\n",
    "print(f'Data directory: {DATA_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b06452b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_df = pd.read_csv(f'{DATA_DIR}/train.csv')\n",
    "test_df = pd.read_csv(f'{DATA_DIR}/test.csv')\n",
    "\n",
    "print(f'Train shape: {train_df.shape}')\n",
    "print(f'Test shape: {test_df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0068be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering Class\n",
    "class FeatureEngineer:\n",
    "    def __init__(self):\n",
    "        self.imputer = SimpleImputer(strategy='median')\n",
    "        self.scaler = StandardScaler()\n",
    "        self.feature_cols = None\n",
    "        \n",
    "    def fit(self, df, feature_cols):\n",
    "        self.feature_cols = feature_cols\n",
    "        X = df[feature_cols].values\n",
    "        X_imputed = self.imputer.fit_transform(X)\n",
    "        self.scaler.fit(X_imputed)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        X = df[self.feature_cols].values\n",
    "        X_imputed = self.imputer.transform(X)\n",
    "        X_scaled = self.scaler.transform(X_imputed)\n",
    "        return X_scaled\n",
    "    \n",
    "    def fit_transform(self, df, feature_cols):\n",
    "        self.fit(df, feature_cols)\n",
    "        return self.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a965e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trading Strategy Class\n",
    "class TradingStrategy:\n",
    "    def __init__(self, method='sigmoid', scale=100):\n",
    "        self.method = method\n",
    "        self.scale = scale\n",
    "        \n",
    "    def predict_to_weight(self, predictions):\n",
    "        if self.method == 'sigmoid':\n",
    "            weights = 2 / (1 + np.exp(-self.scale * predictions))\n",
    "        elif self.method == 'threshold':\n",
    "            weights = np.where(predictions > 0, 1.5, 0.5)\n",
    "        else:\n",
    "            weights = np.ones_like(predictions)\n",
    "        \n",
    "        return np.clip(weights, MIN_INVESTMENT, MAX_INVESTMENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b487dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features\n",
    "target_cols = ['forward_returns', 'risk_free_rate', 'market_forward_excess_returns']\n",
    "id_col = 'date_id'\n",
    "exclude_cols = [id_col] + target_cols\n",
    "\n",
    "feature_cols = [col for col in train_df.columns if col not in exclude_cols]\n",
    "print(f'Number of features: {len(feature_cols)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5ebdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter training data - use only complete data\n",
    "missing_by_date = train_df[feature_cols].isnull().sum(axis=1)\n",
    "threshold = len(feature_cols) * 0.05\n",
    "\n",
    "valid_mask = missing_by_date <= threshold\n",
    "valid_start_idx = valid_mask.idxmax()\n",
    "valid_start_date = train_df.loc[valid_start_idx, 'date_id']\n",
    "\n",
    "train_clean = train_df[train_df['date_id'] >= valid_start_date].copy().reset_index(drop=True)\n",
    "print(f'Training samples after filtering: {len(train_clean)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec5c306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data\n",
    "fe = FeatureEngineer()\n",
    "X_train = fe.fit_transform(train_clean, feature_cols)\n",
    "y_train = train_clean['forward_returns'].values\n",
    "\n",
    "print(f'X_train shape: {X_train.shape}')\n",
    "print(f'y_train shape: {y_train.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40f3087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "if HAS_LGB:\n",
    "    model = lgb.LGBMRegressor(\n",
    "        n_estimators=200,\n",
    "        max_depth=5,\n",
    "        learning_rate=0.05,\n",
    "        random_state=42,\n",
    "        verbose=-1\n",
    "    )\n",
    "else:\n",
    "    model = GradientBoostingRegressor(\n",
    "        n_estimators=200,\n",
    "        max_depth=3,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "print('Model trained!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad1be07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test data and predict\n",
    "X_test = fe.transform(test_df)\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Convert predictions to weights\n",
    "strategy = TradingStrategy(method='sigmoid', scale=100)\n",
    "weights = strategy.predict_to_weight(predictions)\n",
    "\n",
    "print(f'Prediction statistics:')\n",
    "print(f'  Min weight: {weights.min():.4f}')\n",
    "print(f'  Max weight: {weights.max():.4f}')\n",
    "print(f'  Mean weight: {weights.mean():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "316823a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Submission saved!\n",
      "Files created: submission.parquet, submission.csv\n",
      "   date_id  prediction\n",
      "0     8980    1.044841\n",
      "1     8981    0.939873\n",
      "2     8982    1.004978\n",
      "3     8983    1.032191\n",
      "4     8984    1.036507\n",
      "5     8985    1.051904\n",
      "6     8986    1.027712\n",
      "7     8987    1.038318\n",
      "8     8988    1.024831\n",
      "9     8989    0.961920\n"
     ]
    }
   ],
   "source": [
    "# Create submission\n",
    "submission = pd.DataFrame({\n",
    "    'date_id': test_df['date_id'],\n",
    "    'prediction': weights\n",
    "})\n",
    "\n",
    "# Ensure weights are within valid range\n",
    "submission['prediction'] = submission['prediction'].clip(MIN_INVESTMENT, MAX_INVESTMENT)\n",
    "\n",
    "# Save submission as parquet (required by Kaggle)\n",
    "# Use pyarrow directly to avoid pandas compatibility issues\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "table = pa.Table.from_pandas(submission, preserve_index=False)\n",
    "pq.write_table(table, 'submission.parquet')\n",
    "\n",
    "# Also save as CSV for local reference\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print('\\nSubmission saved!')\n",
    "print('Files created: submission.parquet, submission.csv')\n",
    "print(submission.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1d4328b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Submission verification:\n",
      "  Rows: 10\n",
      "  Columns: ['date_id', 'prediction']\n",
      "  Any NaN: False\n",
      "  Weight range: [0.9399, 1.0519]\n",
      "\n",
      "✓ submission.parquet created successfully!\n",
      "  File size: 1943 bytes\n"
     ]
    }
   ],
   "source": [
    "# Verify submission format\n",
    "print('\\nSubmission verification:')\n",
    "print(f'  Rows: {len(submission)}')\n",
    "print(f'  Columns: {list(submission.columns)}')\n",
    "print(f'  Any NaN: {submission.isnull().any().any()}')\n",
    "print(f'  Weight range: [{submission[\"prediction\"].min():.4f}, {submission[\"prediction\"].max():.4f}]')\n",
    "\n",
    "# Verify parquet file exists\n",
    "import os\n",
    "if os.path.exists('submission.parquet'):\n",
    "    print('\\n✓ submission.parquet created successfully!')\n",
    "    parquet_size = os.path.getsize('submission.parquet')\n",
    "    print(f'  File size: {parquet_size} bytes')\n",
    "else:\n",
    "    print('\\n✗ ERROR: submission.parquet not found!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
